{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"top\"></a>\n",
    "# Generating TESS Cutouts Quickly with TIKE\n",
    "***\n",
    "The Timeseries Integrated Knowledge Engine (TIKE) is a cloud computing environment offered by the MAST Archive. You should already be inside this environment if you're running this notebook!\n",
    "\n",
    "One advantage of working in the cloud is rapid access to data. There's no need to submit a query to the MAST servers in Baltimore and wait for a download to your local machine to complete. Instead, you can work with the data directly in the cloud.\n",
    "\n",
    "To that end, this tutorial will demonstrate:\n",
    "\n",
    "- Filtering the TESS Threshold Crossing Events (TCE) catalog for transits of interest.\n",
    "- Using TESSCut to get data from a target of interest.\n",
    "- Accessing cutouts more quickly and efficiently using cutout cubes.\n",
    "- Optimizing our cloud query for better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "The [TESS Mission](https://archive.stsci.edu/missions-and-data/tess) was launched in 2018 to find nearby transiting exoplanets. Its large, $24^\\circ$ x $96^\\circ$ field-of-view allows for simultaneous observations of a large part of the sky. These Full Field Images (FFIs) are tremendously useful for science. Although local [Target Pixel Files](https://heasarc.gsfc.nasa.gov/docs/tess/data-products.html) (AKA \"postage stamps\") are produced for many stars TESS observes, some potential targets are only available in the FFIs.\n",
    "\n",
    "[Starting in September 2023](https://tess.mit.edu/news/tess-begins-its-second-extended-mission/), TESS will take a FFI every 200 seconds. This is a significant change from the 30 minute and 10 minute intervals in the prime mission and first extended mission. One challenge with this upgraded FFI frequency is the increased data volume. For example, the calibrated Sector 56 FFIs are **6.7 TB**. This exceeds the storage capabilities of most computers, in addition to being a drain on network and computing resources. \n",
    "\n",
    "So how can you do science in the era of big data? Rather than requesting all of this data from the [MAST Archive](https://archive.stsci.edu) servers in Baltimore, you can work within a cloud environment. In a nutshell, rather than downloading the data, you can upload your code. This is where TIKE comes in. Making [TESS data available on AWS](https://registry.opendata.aws/tess/) allows for _very_ rapid access to data, regardless of your own download speeds or computer performance.\n",
    "\n",
    "The workflow for this notebook consists of:\n",
    "* [Selecting Targets of Interest](#Targets-of-Interest)\n",
    "* [Cutout Method 1: TESSCut in the Cloud](#Method1)\n",
    "* [Cutout Method 2: Cloud Cutout Cubes](#Method2)\n",
    "* [Cutout Method 3: Cutout Cubes with Multiple Cores](#Method3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Imports\n",
    "We'll need a few unusual imports for this notebook to function properly. They should all be pre-installed on the TESS Environment kernel in TIKE.\n",
    "- `Astrocut`, which will let us work with pre-generated cutout cubes\n",
    "- `nest_asyncio` to use AWS S3 within the notebook environment\n",
    "- We'll import the `multiprocessing` module to run our program on multiple cores. To run this within a Jupyter Notebok, we need to import `multi` file from within this folder; `multi` contains some convenience functions we'll write in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import multi as m\n",
    "import multiprocessing\n",
    "import nest_asyncio\n",
    "import pandas as pd\n",
    "import requests\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from astrocut import CutoutFactory\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astroquery.mast import Tesscut "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Targets of Interest\n",
    "\n",
    "We'll find our targets using the [TESS TCE bulk download](https://archive.stsci.edu/tess/bulk_downloads/bulk_downloads_tce.html) bulk download list. We'll limit ourselves to Sector 55 to keep the number of results low, and our runtimes short. However, this technique will work on any sector!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the CSV as a pandas array. Ignore the first five rows of the CSV, which are comments\n",
    "tess = pd.read_csv('tess55.csv', header=6)\n",
    "\n",
    "# Display the first five rows of the array, to make sure the file was read correctly\n",
    "tess.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a Subset\n",
    "\n",
    "Let's only look for transits that display a large dip in brightness. `tce_depth` is given in in parts per million. Here, we're selecting truly enormous dips, at a million parts per million. \n",
    "\n",
    "\"Now wait a minute\", you might be saying, \"wouldn't that mean a star is repeatedly dimming by 100% of its brighness?\". And indeed, that is exactly the criterion we are using to filter. A star which actually turned on and off like a lightbulb would be the astrophysical discovery of the century; these high values are likely due to errors in data processing. However, we might be able to \"clean\" some of the data and recover a more sensible result. Stay tuned for a follow-up notebook.\n",
    "\n",
    "Note that we're going to drop duplicates from our list. Multiple entries might indicate, for example, a star system with multiple planets. For our purpose of selecting target stars, this is not relevant, so we will ignore this information (for now)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select only depths > 1 million ppm\n",
    "large_depth = tess[tess['tce_depth']>1000000]\n",
    "\n",
    "# Drop duplicate stars\n",
    "large_depth = large_depth.drop_duplicates(subset = \"ticid\")\n",
    "\n",
    "len(large_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview some of the results, if you like\n",
    "large_depth[['ticid', 'tce_depth', 'tce_period']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convenience Functions\n",
    "As we proceed through the notebook, it will be helpful to have coordinates associated with our targets. We'll also need an easy way to determine the camera and CCD the targets were observed on. For this, we'll create a dictionary where each target name is associated with the values we want.\n",
    "\n",
    "Let's create a helper function to generate the dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDict(target_table):\n",
    "    '''\n",
    "    Create a dictionary where the keys are TESS IDs, and the values are\n",
    "    the coordinates, camera, and CCD.\n",
    "    Input: Table of targets with a column named 'ticid' containing the TESS IDs\n",
    "    Output: Dictionary of those TESS IDs and the associated coordinates, camera, and CCD\n",
    "    '''\n",
    "    # Create a \"results\" dictionary that stores the target name, coordinates, and camera/CCD\n",
    "    nameCoordCam = dict()\n",
    "    \n",
    "    # Loop through the TESS IDs in the target table\n",
    "    for tid in target_table['ticid']:\n",
    "        # Add 'TIC' to the target name\n",
    "        target_name = f\"TIC{tid}\"\n",
    "        \n",
    "        # Get the coordinates for the target\n",
    "        target_crd = SkyCoord.from_name(target_name)\n",
    "        \n",
    "        # Get the camera/CCD used to observe the target. We still need to write this function.\n",
    "        cam_ccd = getCamCCD(target_crd)\n",
    "        \n",
    "        # Add an entry to the dictionary to store the above information\n",
    "        nameCoordCam[target_name] = [target_crd, cam_ccd]\n",
    "        \n",
    "    return nameCoordCam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this function is a pretty handy dictionary. For any target on our list, we have now saved its coordinates and the camera/CCD used by TESS during the observation. We'll need this information when we request cutout cubes in methods 2 and 3.\n",
    "\n",
    "The `makeDict()` function is only six lines of code because we've hidden a relatively complex operation under the guise of the `getCamCCD()` function. This is actually non-trivial to do. This function needs to find the the camera and CCD used for each observation, which we'll do by querying the TESSCut API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getCamCCD(coord):\n",
    "    '''\n",
    "    For a given set of coordinates, return the camera/CCD used for observation.\n",
    "    Input: astropy.coordinate object\n",
    "    Output: string formatted like \"[camera]-[CCD]\"\n",
    "    '''\n",
    "    \n",
    "    # Split the coordinate object into Ra/Dec\n",
    "    Ra = coord.ra.degree\n",
    "    Dec = coord.dec.degree\n",
    "    \n",
    "    # Our parameters are the Ra/Dec, which we want to match exactly (zero-radius)\n",
    "    myparams = {\"ra\":Ra, \"dec\":Dec, \"radius\":\"0m\"}\n",
    "    \n",
    "    # This could be one line of code, but it's good to remember you can query the API for cutouts directly\n",
    "    urlroot = \"https://mast.stsci.edu/tesscut/api/v0.1\"\n",
    "    url = urlroot + \"/sector\"\n",
    "    \n",
    "    # Run the request, get results in JSON format\n",
    "    requestData = requests.get(url = url, params = myparams)\n",
    "    results = requestData.json()['results']\n",
    "    \n",
    "    # Our results have multiple sectors, loop through them and keep only sector 55.\n",
    "    for result in results:\n",
    "        if result['sector'] == '0055':\n",
    "            # Get the camera/CCD, then format them \"nicely\"\n",
    "            camnum = result['camera']\n",
    "            ccdnum = result['ccd']\n",
    "            combined = f\"{camnum}-{ccdnum}\"\n",
    "    \n",
    "    return combined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great! We've written all the helper functions we need to create the dictionary. Let's run our function and take a look at the first entry.\n",
    "\n",
    "**A note on optimization**: Although this solution is \"quick enough\" for this example, each call to the API will take roughly 0.2 seconds to complete. If you try to run this on a million objects, you'll be sitting around for 55 hours!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the \"large depth\" transits dictionary\n",
    "ld_dict = makeDict(large_depth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print only the first entry from our dictionary\n",
    "for k, v in ld_dict.items():\n",
    "    print(f\"Key: {k}\\nValue: {v}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Printing out the first key/value pair makes it clear exactly why this is useful to us. We now have a list of TESS targets associated with their coordinates and the camera/CCD on which they were observed.\n",
    "\n",
    "Some of our queries, particularly methods 2 and 3, will be much easier to run using this dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Method1\"></a>\n",
    "# Method 1: TESSCut in the Cloud\n",
    "We'll start by testing out the \"conventional\" method, which makes use of the [TESSCut](https://mast.stsci.edu/tesscut/) function. This is a convenient way to generate cutouts, and can be run on your local machine.\n",
    "\n",
    "Let's generate cutouts for all of our targets and time how long it takes to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Record the start time so we can see how long this takes\n",
    "t0 = time.time()\n",
    "\n",
    "# Go through our dictionary values, which contain the coordinates and cam/ccd\n",
    "for v in ld_dict.values():\n",
    "    # Get the coordinates, then pass them to get_cutouts()\n",
    "    coord = v[0]\n",
    "    hdulist = Tesscut.get_cutouts(coordinates=coord, size=10, sector=55)\n",
    "\n",
    "# Print the time spent executing this cell\n",
    "print(f\"Took {time.time()-t0:.1f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the TIKE environment, this takes about 40 seconds. If you were to run this request on your local machine, it would take around 380 seconds; nearly 10 times longer!\n",
    "\n",
    "This performance improvement is extremely impressive when you consider that all we've done is run our code in TIKE. For many users, this convenient factor of ten speedup may be more than fast enough. However, for large enough queries, it might be worthwhile to continue optimizing. Let's explore some options for further improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"Method2\"></a>\n",
    "# Method 2: Cloud Cutout Cubes\n",
    "A \"[TESS Cutout Cube](https://astrocut.readthedocs.io/en/latest/astrocut/index.html#tess-full-frame-image-cutouts)\" is an image cube which MAST has generated from the TESS FFIs for one particular sector, camera, and CCD. These files are not meant to be downloaded, as they often exceed 100GB in size. However, these cubes are a _very efficient_ way to generate cutouts; indeed, this is what TESSCut is doing behind the scenes. Let's write our own `get_cutouts` function and see if we can get the data faster.\n",
    "\n",
    "Since the cutout cubes are specific to a camera, sector, and CCD, we need this information for each target. This is where the dictionary we built comes in handy, as it will help us match our targets to their corresponding cube files. Let's build another helper function to generate these cube file names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cube_fn(camera_ccd, sector=55):  # Note that the default sector is 55 to match our example.\n",
    "    # In the filename, the sector has two leading zeros, i.e. 0055\n",
    "    cube_fn = f\"s3://stpubdata/tess/public/mast/tess-s{sector:04d}-{camera_ccd}-cube.fits\"\n",
    "    return cube_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost done! Let's create one more function takes in our target dictionary and gets all of the cutouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cutouts(target_dict):\n",
    "    # We need nest_asyncio for AWS access within Jupyter\n",
    "    nest_asyncio.apply()\n",
    "    # Create a \"cutout factory\" to generate cutouts\n",
    "    factory = CutoutFactory()\n",
    "    for k, v in target_dict.items():\n",
    "        # Get the coordinates and camera/ccd info from the dictionary\n",
    "        coord = v[0]\n",
    "        cam_ccd = v[1]\n",
    "        print(f\"Starting {k}\")\n",
    "        factory.cube_cut(cube_file = get_cube_fn(cam_ccd) # Get the cube filename for this camera/ccd\n",
    "                        ,coordinates=coord                # Use the coordinates for the target\n",
    "                        ,cutout_size=10\n",
    "                        ,target_pixel_file=f\"{k}.fits\");  # Name the output file the target name\n",
    "        print(f\"Finished {k}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're now ready to run our function and generate the cutouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We're ignoring some warnings about the size of the comment card\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Start time for this cell\n",
    "t0 = time.time()\n",
    "\n",
    "# Get the cutouts for the large-depth dictionary\n",
    "get_cutouts(ld_dict)\n",
    "\n",
    "# Print how long this cell took to run\n",
    "print(f\"Took {time.time()-t0:.1f} seconds.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This usually runs in under 20 seconds; half the time of Method 1!\n",
    "\n",
    "However, there's a way to squeeze out even more performance from this query. Note the order in which this ran; we start the request for a cutout, then we wait for the data to be returned. Wouldn't it be nice if we could request multiple cutouts at the same time?\n",
    "\n",
    "As a matter of fact, we can do exactly that with multiprocessing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 3: Cutout Cubes with Multiple Cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Python has a feature unique among programming languages called the Global Interpreter Lock, or GIL. You can read [this interesting blog post](https://realpython.com/python-gil/) for more details, but in essence: the GIL prevents more than one Python thread from running at the same time. This matters in this example, because we can only request one cutout at a time. To be most efficient, every CPU core should have its own thread, so they can all work on getting cutouts.\n",
    "\n",
    "We can figure out how many processors are available with a call to the `cpu_count` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cores = multiprocessing.cpu_count()\n",
    "n_cores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing, TIKE offers us 4 processors. Thus we could, in theory, cut down on the run time by a factor of 4. In practice, the speedup will be slightly less than this since multiprocessing is not perfectly efficient.\n",
    "\n",
    "We'll need to make some minor changes to our functions in order for them to run on multiple processors. To start, we need to break down our single list of targets into four lists, one for each processor. We want these lists close in length to each other, so that each processor handles roughly the same number of cutouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the list of targets out of our dictionary\n",
    "targets = list(ld_dict.keys())\n",
    "\n",
    "# Assign targets so that each CPU has a list of roughly equal length\n",
    "target_lists = [targets[i::n_cores] for i in range(n_cores)]\n",
    "target_lists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lists aren't perfectly equal lengths, but they're close enough to maximize multiprocessing efficiency.\n",
    "\n",
    "Our next big change will be how we call the functions. Unfortunately, the multiprocessing module was not designed to be run in a Jupyter Notebook. As a workaround, we can copy the code we wrote before to a separate python file and import it. \n",
    "\n",
    "We are doing this because `multiprocessing` expects to see `if __name__=='main':`, but this syntax does not function properly in a notebook environment. Ideally, multiprocessing code would be contained to its own Python script; see examples of proper use in the [Python multiprocessing docs](https://docs.python.org/3/library/multiprocessing.html#module-multiprocessing).\n",
    "\n",
    "We're now ready to run our query with multiple processors. Watch the output of the below cell and note that the queries do not execute linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import our functions, and ignore warnings\n",
    "import multi as m\n",
    "warnings.simplefilter('ignore')\n",
    "\n",
    "# Time this method\n",
    "t0 = time.time()\n",
    "\n",
    "# Create a list to hold CPU processes\n",
    "processes = []\n",
    "\n",
    "# Go through the lists of targets, and start one process for each\n",
    "for target_list in target_lists:\n",
    "    # The \"target\" of multiprocessing is our cutout function.\n",
    "    # The arguments are our target list and target dictionary\n",
    "    a_process = multiprocessing.Process(target = m.get_cutouts\n",
    "                                        ,args = [target_list, ld_dict])\n",
    "    a_process.start()\n",
    "    processes.append(a_process)\n",
    "\n",
    "# Join the processes, which ends them and stops them from becoming \"zombies\"\n",
    "for a_process in processes:\n",
    "      a_process.join()\n",
    "           \n",
    "# Print the time taken to run this cell            \n",
    "print(f\"Took {time.time()-t0:.1f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With multiple processors, it takes under 7 seconds to get the cutouts. This is about three times faster than the previous method! \n",
    "\n",
    "**A note on runtime, CPUs, and processors:** We might have expected a time closer to 5 seconds: that's the Method #2 result of 20 seconds divided by 4. However, our \"4 cores\" actually consist of 2 physical cores and 2 virtual cores. Since we need physical silicon to perform calculations, we cannot run four processes simultaneously; there are only two physical cores available. With that said, it's still worthwhile to use the virtual cores: the CPU will use [multi-threading](https://en.wikipedia.org/wiki/Multithreading_(computer_architecture)) to save an additional second or two.\n",
    "\n",
    "# Summary and Key Takeaways\n",
    "We've gone over several methods for generating cutouts from FFIs. Let's compare the performance for each method:\n",
    "\n",
    "Method | Time to Run (s) | Time per Cutout (s)\n",
    ":--- | :---: | :---:\n",
    "Local Machine, TESSCut | 380 | 29.2\n",
    "TIKE, TESSCut | 40 | 3.1\n",
    "TIKE, Cutout Cubes | < 20 | < 1.5\n",
    "TIKE, Multiprocessing & Cubes | < 7 | < 0.5\n",
    "\n",
    "It is worth pointing out that our best time is nearly 60x faster than doing a \"standard\" TESSCut. The multiprocessing method is the trickiest to implement, but can be worthwhile if you're generating a large number of cutouts.\n",
    "\n",
    "Even with no changes to the code, using the TIKE platform results in a significant speed improvement. This is true for many packages, including `astroquery`, `lightkurve`, `astrocut`, and others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Resources\n",
    "\n",
    "- [TESS Archive Manual](https://outerspace.stsci.edu/display/TESS/TESS+Archive+Manual) for more information about the TESS data products stored in MAST\n",
    "- [TESSCut Home](https://mast.stsci.edu/tesscut/), for additional reading about the TESSCut software"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## About this Notebook\n",
    "For support, please contact the Archive HelpDesk at archive@stsci.edu, or through the [MAST HelpDesk Portal](https://stsci.service-now.com/mast).\n",
    "\n",
    "**Author:** Thomas Dutkiewicz <br>\n",
    "**Keywords:** TIKE, AWS Cloud, Muliprocessing, Optimization <br>\n",
    "**Last Updated:** Nov 2022 <br>\n",
    "**Next Review:** Apr 2023\n",
    "***\n",
    "[Top of Page](#top)\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/spacetelescope/notebooks/master/assets/stsci_pri_combo_mark_horizonal_white_bkgd.png\" alt=\"Space Telescope Logo\" width=\"200px\"/> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "TESS Environment",
   "language": "python",
   "name": "tess"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
